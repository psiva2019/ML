{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sivaramakrishnan/Development/ML/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/sivaramakrishnan/Development/ML/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/sivaramakrishnan/Development/ML/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForTokenClassification, pipeline\n",
    "\n",
    "# Load the pre-trained RoBERTa tokenizer and model for token classification\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n",
    "model = RobertaForTokenClassification.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n",
    "\n",
    "# Alternatively, you can use the Hugging Face pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\",device=\"mps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity:  Dubai, Label: LOC, Confidence: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"The invoice number is INV12345 and the total amount is $1500. Dubai pay on 10th May 2022 pay to account number 1234567890.\"\n",
    "\n",
    "# Use the pipeline to extract entities\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "# Print out the entities\n",
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Confidence: {entity['score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1:\n",
      "  Invoice Number: INV-12345\n",
      "  Invoice Amount: 120\n",
      "  Tax Amount: 10\n",
      "  Total Amount: 120\n",
      "  Date: 20 July 2024\n",
      "\n",
      "Text 2:\n",
      "  Invoice Number: is\n",
      "  Invoice Amount: 150\n",
      "  Total Amount: 150\n",
      "  Date: 15 August 2024\n",
      "\n",
      "Text 3:\n",
      "  Invoice Number: INV-99999\n",
      "  Invoice Amount: 200\n",
      "  Total Amount: None\n",
      "  Date: 01 September 2024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Dict, List\n",
    "\n",
    "def extract_entities(text: str) -> Dict[str, str]:\n",
    "    # Define regex patterns for extraction\n",
    "    patterns = {\n",
    "        'Invoice Number': r'(?:Invoice Number|Invoice ID|Invoice number):?\\s*([A-Z0-9-]+)',\n",
    "        'Invoice Amount': r'Rs\\s*(\\d+)(?:\\s*(?:for|invoice|amount))?',\n",
    "        'Tax Amount': r'tax\\s*of?\\s*Rs\\s*(\\d+)',\n",
    "        'Total Amount': r'total amount\\s*of?\\s*Rs\\s*(\\d+)|total payment due is Rs\\s*(\\d+)|Pay Rs\\s*(\\d+)',\n",
    "        'Date': r'dated\\s*(\\d{1,2}\\s\\w+\\s\\d{4})|The date is (\\d{1,2}\\s\\w+\\s\\d{4})'\n",
    "    }\n",
    "    \n",
    "    entities = {}\n",
    "    \n",
    "    for entity, pattern in patterns.items():\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            # Extract the first group from the match\n",
    "            entities[entity] = match.group(1) if match.group(1) else match.group(2)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Example texts\n",
    "texts = [\n",
    "    \"Please pay the total amount of Rs 120, which includes the invoice amount of Rs 100 and tax of Rs 10, for Invoice Number INV-12345 dated 20 July 2024, to Siva & Co. The payment should be made to account number 12345678 at ABC Bank, Chennai.\",\n",
    "    \"The total payment due is Rs 150. This includes Rs 120 for the invoice and Rs 30 for tax. Invoice ID is INV-67890 dated 15 August 2024. Account 87654321 at XYZ Bank, Mumbai.\",\n",
    "    \"Pay Rs 200 total. Invoice was Rs 150 and tax Rs 50. Invoice number: INV-99999. The date is 01 September 2024. Account 98765432, Bank: DEF Bank, Bangalore.\"\n",
    "]\n",
    "\n",
    "# Extract entities from each text\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Text {i + 1}:\")\n",
    "    entities = extract_entities(text)\n",
    "    for key, value in entities.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best match for 'total amount': 'please' with similarity 0.4684\n",
      "Best match for 'invoice amount': '##ice' with similarity 0.6334\n",
      "Best match for 'tax': '.' with similarity 0.5107\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def get_embeddings(sentence, model, tokenizer):\n",
    "    # Tokenize and encode the sentence\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the embeddings from the last hidden layer\n",
    "    layer_embeddings = outputs.last_hidden_state\n",
    "    # Average pooling to get sentence embedding\n",
    "    sentence_embedding = torch.mean(layer_embeddings, dim=1).squeeze().numpy()\n",
    "    \n",
    "    # Token embeddings\n",
    "    token_embeddings = layer_embeddings.squeeze().numpy()\n",
    "    \n",
    "    # Extract tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze().tolist())\n",
    "    \n",
    "    return tokens, token_embeddings, sentence_embedding\n",
    "\n",
    "def get_phrase_embedding(phrase, model, tokenizer):\n",
    "    # Tokenize and encode the phrase\n",
    "    inputs = tokenizer(phrase, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the embeddings from the last hidden layer\n",
    "    layer_embeddings = outputs.last_hidden_state\n",
    "    # Average pooling to get phrase embedding\n",
    "    phrase_embedding = torch.mean(layer_embeddings, dim=1).squeeze().numpy()\n",
    "    \n",
    "    return phrase_embedding\n",
    "\n",
    "def find_best_match(target_phrase, sentence, model, tokenizer):\n",
    "    # Compute embeddings for the entire sentence\n",
    "    tokens, token_embeddings, sentence_embedding = get_embeddings(sentence, model, tokenizer)\n",
    "    \n",
    "    # Compute embedding for the target phrase\n",
    "    target_embedding = get_phrase_embedding(target_phrase, model, tokenizer)\n",
    "    \n",
    "    # Calculate the similarity between the target phrase embedding and all token embeddings\n",
    "    highest_similarity = -1\n",
    "    best_match = None\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        if token not in tokenizer.all_special_tokens and token.strip():\n",
    "            token_embedding = token_embeddings[i].reshape(1, -1)  # Reshape for cosine_similarity\n",
    "            target_embedding_reshaped = target_embedding.reshape(1, -1)  # Reshape for cosine_similarity\n",
    "            \n",
    "            # Calculate similarity between target phrase and each token\n",
    "            token_similarity = calculate_similarity(target_embedding_reshaped, token_embedding)\n",
    "            \n",
    "            if token_similarity > highest_similarity:\n",
    "                highest_similarity = token_similarity\n",
    "                best_match = token\n",
    "    \n",
    "    return best_match, highest_similarity\n",
    "\n",
    "def calculate_similarity(embedding1, embedding2):\n",
    "    # Ensure that the embeddings are not NaN\n",
    "    if np.isnan(embedding1).any() or np.isnan(embedding2).any():\n",
    "        print(\"Warning: One of the embeddings contains NaN values. Similarity will be set to 0.\")\n",
    "        return 0.0\n",
    "    \n",
    "    return cosine_similarity(embedding1, embedding2)[0][0]\n",
    "\n",
    "# Load the BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "# Define the sentence\n",
    "sentence = \"Please pay the total amount of Rs 120, which includes the invoice amount of Rs 100 and tax of Rs 10.\"\n",
    "\n",
    "# Example target phrases\n",
    "targets = [\"total amount\", \"invoice amount\", \"tax\"]\n",
    "\n",
    "for target in targets:\n",
    "    match, similarity = find_best_match(target, sentence, model, tokenizer)\n",
    "    print(f\"Best match for '{target}': '{match}' with similarity {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
